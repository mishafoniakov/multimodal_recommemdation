{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Creating clusters of images, using CLIP and BERT vectors and count metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we work with CLIP and BERT vectors and try to create clusters and count metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Representation of classes and functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset class: it is responsible for all operations with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def json_file_opening(self, json_file, img, feature_1, feature_2, \n",
    "                          dictionary=False, reshaping=False, tensor=False):\n",
    "        with open(json_file, 'r') as file:\n",
    "            img_features = [(json.loads(line)[img],\n",
    "                 json.loads(line)[feature_1],\n",
    "                 json.loads(line)[feature_2])\n",
    "                  for line in file]\n",
    "        img_list = [feature[0] for feature in img_features]\n",
    "        feature_1_list = [feature[1] for feature in img_features]\n",
    "        feature_2_list = [feature[2] for feature in img_features]\n",
    "        if dictionary == False:\n",
    "            return img_list, feature_1_list, feature_2_list\n",
    "        else:\n",
    "            dict_1_list, dict_2_list = {}, {}\n",
    "            assert len(img_list) == len(feature_1_list)\n",
    "            assert len(img_list) == len(feature_2_list)\n",
    "            for i in range(len(img_list)):\n",
    "                if reshaping == True:\n",
    "                    dict_1_list[img_list[i]] = np.array(feature_1_list[i]).reshape(1, -1)\n",
    "                    dict_2_list[img_list[i]] = np.array(feature_2_list[i]).reshape(1, -1)\n",
    "                elif tensor == True:\n",
    "                    dict_1_list[img_list[i]] = torch.FloatTensor(feature_1_list[i]).unsqueeze(dim=0)\n",
    "                    dict_2_list[img_list[i]] = torch.FloatTensor(feature_2_list[i]).unsqueeze(dim=0)\n",
    "                else:\n",
    "                    dict_1_list[img_list[i]] = feature_1_list[i]\n",
    "                    dict_2_list[img_list[i]] = feature_2_list[i]\n",
    "            return dict_1_list, dict_2_list\n",
    "    \n",
    "    def split_tensors(self, dataframe):\n",
    "        dataframe['clip'] = dataframe['clip'].apply(lambda x: torch.FloatTensor(x))\n",
    "        dataframe['bert'] = dataframe['bert'].apply(lambda x: torch.FloatTensor(x))\n",
    "        dataframe['clip'] = dataframe['clip'].apply(lambda x: x.squeeze().numpy())\n",
    "        dataframe['bert'] = dataframe['bert'].apply(lambda x: x.squeeze().numpy())\n",
    "        columns_clip = [f'clip_{i}' for i in range(dataframe['clip'][0].shape[0])]\n",
    "        columns_bert = [f'bert_{i}' for i in range(dataframe['bert'][0].shape[0])]\n",
    "        split_img_clip = pd.DataFrame(dataframe['clip'].tolist(), columns = columns_clip)\n",
    "        split_img_bert = pd.DataFrame(dataframe['bert'].tolist(), columns = columns_bert)\n",
    "        dataframes = [split_img_clip, split_img_bert]\n",
    "        dataframe = pd.concat(dataframes, axis=1)\n",
    "        return dataframe\n",
    "    \n",
    "    def clip_bert_vectors(self, dataframe, clip_dict, bert_dict):\n",
    "        dataframe['img_clip_1'] = dataframe['image'].apply(lambda x: clip_dict.get(x))\n",
    "        dataframe['img_clip_2'] = dataframe['candidate'].apply(lambda x: clip_dict.get(x))\n",
    "        dataframe['img_bert_1'] = dataframe['image'].apply(lambda x: bert_dict.get(x))\n",
    "        dataframe['img_bert_2'] = dataframe['candidate'].apply(lambda x: bert_dict.get(x))\n",
    "        return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Analytics class: it is responsible for all analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1D(series):\n",
    "    return [x for _list in series for x in _list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analytics:\n",
    "    def __init__(self, img_list):\n",
    "        self.img_list = img_list\n",
    "    \n",
    "    def nearest_clusters(self, img_number, clusters_number, nearest, clusters, nearest_clusters_number):\n",
    "        nearest_clusters_dict = {}\n",
    "        with open('nearest_clusters.json', 'w') as near:\n",
    "            pass\n",
    "        images_vect = [0] * img_number\n",
    "        for i in range(img_number):\n",
    "            images_clust = [0] * clusters_number\n",
    "            for j in range(clusters_number):\n",
    "                vector = nearest[i][clusters[i]]\n",
    "                images_clust[j] = abs(vector - nearest[i][j])\n",
    "            images_vect[i] = images_clust\n",
    "        images_sorted = [0] * img_number\n",
    "        for i in range(img_number):\n",
    "            images_sorted[i] = sorted(images_vect[i])[:nearest_clusters_number]\n",
    "            nearest_clusters = [0] * img_number\n",
    "        for i in range(img_number):\n",
    "            clusters_per_img = [0] * nearest_clusters_number\n",
    "            for j in range(nearest_clusters_number):\n",
    "                clusters_per_img[j] = images_vect[i].index(images_sorted[i][j])\n",
    "            nearest_clusters[i] = clusters_per_img\n",
    "            with open('nearest_clusters.json', 'a') as near:\n",
    "                nearest_clusters_json = {'img': self.img_list[i], 'clusters': nearest_clusters[i]}\n",
    "                json.dump(nearest_clusters_json, near)\n",
    "                near.write('\\n')\n",
    "            nearest_clusters_dict[self.img_list[i]] = nearest_clusters[i]\n",
    "        return nearest_clusters_dict\n",
    "    \n",
    "    def nearest_clusters_images(self, nearest_cl_dict):\n",
    "        nearest_clusters_images = {}\n",
    "        zip_keys = [(value[0], key) for key, value in nearest_cl_dict.items()]\n",
    "        clusters_images_2 = defaultdict(list)\n",
    "        for key, value in zip_keys:\n",
    "            clusters_images_2[key].append(value)\n",
    "        for key, values in nearest_cl_dict.items():\n",
    "            nearest_clusters_images[key] = list(set(to_1D([clusters_images_2.get(value) for value in values])))\n",
    "        data = {'image': nearest_clusters_images.keys(), 'candidate': nearest_clusters_images.values()}\n",
    "        nearest_clusters_df = pd.DataFrame(data)\n",
    "        nearest_clusters_df = nearest_clusters_df.explode('candidate', ignore_index=True)\n",
    "        nearest_clusters_df = nearest_clusters_df.drop(\n",
    "            nearest_clusters_df.loc[nearest_clusters_df['image'] == nearest_clusters_df['candidate']].index)\n",
    "        nearest_clusters_df.to_csv('nearest_clusters.csv', index=False)\n",
    "        return nearest_clusters_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating clusters of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json files in two formats: as dictionaries and dataframe\n",
    "dataset = Dataset()\n",
    "img_clip, img_bert = dataset.json_file_opening('clip_bert.json', 'img', 'clip', 'bert', \n",
    "                                               dictionary=True, reshaping=False, tensor=True)\n",
    "clip_bert_data = pd.read_json('clip_bert.json')\n",
    "clip_bert = clip_bert_data[['clip', 'bert']]\n",
    "clip_bert = dataset.split_tensors(clip_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we mark necessary metrics\n",
    "assert len(list(img_clip.keys())) == len(list(img_bert.keys()))\n",
    "img_list = list(img_clip.keys())\n",
    "img_number = len(img_list)\n",
    "clusters_number = 1000\n",
    "nearest_clusters_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create clusters and distance to clusters\n",
    "kmeans = KMeans(n_clusters=clusters_number)\n",
    "clusters = kmeans.fit_predict(clip_bert)\n",
    "nearest = kmeans.fit_transform(clip_bert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Counting nearest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = Analytics()\n",
    "# we count nearest clusters for image\n",
    "nearest_clusters = analytics.nearest_clusters(img_number, clusters_number, nearest, clusters, nearest_clusters_number)\n",
    "# we change clusters to images and create dataframe with pairs of images\n",
    "nearest_clusters_df = analytics.nearest_clusters_images(nearest_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making paired dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract clip and bert vectors\n",
    "data = dataset.clip_bert_vectors(nearest_clusters_df, img_clip, img_bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
