{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Counting probabilities using model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict probabilities of our pipeline using model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from sklearn.metrics.pairwise import paired_distances\n",
    "import random\n",
    "from PIL import Image\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset class: it is responsible for all operations with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1D(series):\n",
    "    return [x for _list in series for x in _list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def json_file_opening(self, json_file, img, feature_1, feature_2, \n",
    "                          dictionary=False, reshaping=False, tensor=False):\n",
    "        with open(json_file, 'r') as file:\n",
    "            img_features = [(json.loads(line)[img],\n",
    "                 json.loads(line)[feature_1],\n",
    "                 json.loads(line)[feature_2])\n",
    "                  for line in file]\n",
    "        img_list = [feature[0] for feature in img_features]\n",
    "        feature_1_list = [feature[1] for feature in img_features]\n",
    "        feature_2_list = [feature[2] for feature in img_features]\n",
    "        if dictionary == False:\n",
    "            return img_list, feature_1_list, feature_2_list\n",
    "        else:\n",
    "            dict_1_list, dict_2_list = {}, {}\n",
    "            assert len(img_list) == len(feature_1_list)\n",
    "            assert len(img_list) == len(feature_2_list)\n",
    "            for i in range(len(img_list)):\n",
    "                if reshaping == True:\n",
    "                    dict_1_list[img_list[i]] = np.array(feature_1_list[i]).reshape(1, -1)\n",
    "                    dict_2_list[img_list[i]] = np.array(feature_2_list[i]).reshape(1, -1)\n",
    "                elif tensor == True:\n",
    "                    dict_1_list[img_list[i]] = torch.FloatTensor(feature_1_list[i]).unsqueeze(dim=0)\n",
    "                    dict_2_list[img_list[i]] = torch.FloatTensor(feature_2_list[i]).unsqueeze(dim=0)\n",
    "                else:\n",
    "                    dict_1_list[img_list[i]] = feature_1_list[i]\n",
    "                    dict_2_list[img_list[i]] = feature_2_list[i]\n",
    "            return dict_1_list, dict_2_list\n",
    "        \n",
    "    def json_file_opening_one_sample(self, json_file, attr_1, attr_2, dictionary=False, reshaping=False, tensor=False):\n",
    "        with open(json_file, 'r') as file:\n",
    "            img_features = [(json.loads(line)[attr_1],\n",
    "                 json.loads(line)[attr_2])\n",
    "                  for line in file]\n",
    "        img_list = [feature[0] for feature in img_features]\n",
    "        feature_list = [feature[1] for feature in img_features]\n",
    "        if dictionary == False:\n",
    "            return img_list, feature_list\n",
    "        else:\n",
    "            dict_list = {}\n",
    "            assert len(img_list) == len(feature_list)\n",
    "            for i in range(len(img_list)):\n",
    "                if reshaping == True:\n",
    "                    dict_list[img_list[i]] = np.array(feature_list[i]).reshape(1, -1)\n",
    "                elif tensor == True:\n",
    "                    dict_list[img_list[i]] = torch.FloatTensor(feature_list[i]).unsqueeze(dim=0)\n",
    "                else:\n",
    "                    dict_list[img_list[i]] = feature_list[i]\n",
    "            return dict_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Analytics class: it is responsible for all analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analytics:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def candidates(self, dataframe, embedding, metric, n):\n",
    "        dataframe = dataframe.drop(dataframe.loc[dataframe['image'] == dataframe['candidate']].index)\n",
    "        dataframe = dataframe.drop(dataframe.loc[dataframe[f'{embedding}_{metric}'] == 1].index)\n",
    "        nearest_df = dataframe.reset_index(drop=True)\n",
    "        if metric == 'cossim':\n",
    "            grouping = nearest_df.groupby(['image'])[f'{embedding}_{metric}'].nlargest(n)\n",
    "        elif metric == 'l2':\n",
    "            grouping = nearest_df.groupby(['image'])[f'{embedding}_{metric}'].nsmallest(n)\n",
    "        grouping_dict = grouping.to_dict()\n",
    "        paired_tuples = [(key[0], nearest_df.iloc[key[1]]['candidate']) for key in grouping_dict.keys()]\n",
    "        candidates_dict = defaultdict(list)\n",
    "        for img, cand in paired_tuples:\n",
    "            candidates_dict[img].append(cand)\n",
    "        with open(f'image_candidates_{n}_{metric}.json', 'w') as file:\n",
    "            pass\n",
    "        for key, value in candidates_dict.items():\n",
    "            json_dict = {'img': key, 'candidates': value}\n",
    "            with open(f'image_candidates_{n}_{metric}.json', 'a') as candidates:\n",
    "                json.dump(json_dict, candidates)\n",
    "                candidates.write('\\n')\n",
    "        return candidates_dict\n",
    "    \n",
    "    def candidates_model(self, dataframe, n):\n",
    "        nearest_df = dataframe.reset_index(drop=True)\n",
    "        grouping = nearest_df.groupby(['image'])['pred_1'].nlargest(n)\n",
    "        grouping_dict = grouping.to_dict()\n",
    "        paired_tuples = [(key[0], nearest_df.iloc[key[1]]['candidate']) for key in grouping_dict.keys()]\n",
    "        candidates_dict = defaultdict(list)\n",
    "        for img, cand in paired_tuples:\n",
    "            candidates_dict[img].append(cand)\n",
    "        with open(f'image_model_candidates_{n}.json', 'w') as file:\n",
    "            pass\n",
    "        for key, value in candidates_dict.items():\n",
    "            json_dict = {'img': key, 'candidates': value}\n",
    "            with open(f'image_model_candidates_{n}.json', 'a') as candidates:\n",
    "                json.dump(json_dict, candidates)\n",
    "                candidates.write('\\n')\n",
    "        return candidates_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load csv file with paired images for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('paired_images_texts_clusters.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We substitute CLIP and BERT vectors instead images files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "img_clip, img_bert = dataset.json_file_opening('clip_bert.json', \n",
    "                       'img', 'clip', 'bert',\n",
    "                       dictionary=True, tensor=True, reshaping=False)\n",
    "data['img_clip_1'] = data['image'].apply(lambda x: img_clip.get(x))\n",
    "data['img_clip_2'] = data['candidate'].apply(lambda x: img_clip.get(x))\n",
    "data['img_bert_1'] = data['image'].apply(lambda x: img_bert.get(x))\n",
    "data['img_bert_2'] = data['candidate'].apply(lambda x: img_bert.get(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count metrics: cosine similarity and L2 distance for CLIP and BERT vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = Analytics()\n",
    "data = analytics.metrics_count(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape CLIP and BERT vectors for 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['img_clip_1'] = data['img_clip_1'].apply(lambda x: x.numpy().reshape(512))\n",
    "data['img_clip_2'] = data['img_clip_2'].apply(lambda x: x.numpy().reshape(512))\n",
    "data['img_bert_1'] = data['img_bert_1'].apply(lambda x: x.numpy().reshape(768))\n",
    "data['img_bert_2'] = data['img_bert_2'].apply(lambda x: x.numpy().reshape(768))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement two dataframes from one:\n",
    "1. data_images - for searching candidates for images\n",
    "2. data_pipe - to predict probabilitiesof target using CatBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_images = data[['image', 'candidate', 'clip_cossim', 'clip_l2', 'bert_cossim', 'bert_l2']]\n",
    "data_pipe = data[['img_clip_1', 'img_clip_2', 'img_bert_1', 'img_bert_2', 'clip_cossim', 'clip_l2', 'bert_cossim', 'bert_l2']]\n",
    "sc = StandardScaler()\n",
    "scaled_features = ['clip_cossim', 'clip_l2', 'bert_cossim', 'bert_l2']\n",
    "data_pipe[scaled_features] = sc.fit_transform(data_pipe[scaled_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction probabilities of target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict probabilities of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x3ec7b9190>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost = CatBoostClassifier()\n",
    "catboost.load_model('catboost_model.bin')\n",
    "prediction_proba = catboost.predict_proba(data_pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create two columns for P(y=0) and P(y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_0_proba = [pred[0] for pred in prediction_proba]\n",
    "pred_1_proba = [pred[1] for pred in prediction_proba]\n",
    "data_images['pred_0'] = pred_0_proba\n",
    "data_images['pred_1'] = pred_1_proba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find 5 candidates for model, CLIP and BERT vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_model = analytics.candidates_model(data_images, 5)\n",
    "candidates_clip = analytics.candidates(data_images, 'clip', 'cossim', 5)\n",
    "candidates_bert = analytics.candidates(data_images, 'bert', 'cossim', 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
