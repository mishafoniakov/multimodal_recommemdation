{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Representation image dataset and building CLIP-recommendations\n",
    "Here we work with image dataset and create CLIP vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from transformers import CLIPImageProcessor, CLIPModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of images forlder destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X1v1gUzdro8X"
   },
   "outputs": [],
   "source": [
    "dir_path = 'Dataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Representation of classes and functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset class: it is responsible for all operations with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "whxU4Zi1plLo"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, img_dir_path):\n",
    "        self.img_dir_path = img_dir_path\n",
    "\n",
    "    def img_files(self, n=False):\n",
    "        files = []\n",
    "        for path in os.listdir(self.img_dir_path):\n",
    "            if os.path.isfile(os.path.join(self.img_dir_path, path)):\n",
    "                files.append(path)\n",
    "        if n:\n",
    "            random_files = random.sample(files, n)\n",
    "            return random_files\n",
    "        else:\n",
    "            return files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. CLIP class: it is responsible for building CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sCpzQMSmfgFc"
   },
   "outputs": [],
   "source": [
    "#CLIP model representation\n",
    "model_ID = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_ID)\n",
    "preprocess = CLIPImageProcessor.from_pretrained(model_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6FpSwcpjcseB"
   },
   "outputs": [],
   "source": [
    "class CLIP:\n",
    "    def __init__(self, image_list):\n",
    "        self.image_list = image_list\n",
    "    \n",
    "    def get_features(self):\n",
    "        n = len(self.image_list)\n",
    "        for i in range(n):\n",
    "            try:\n",
    "                image = Image.open(f'{dir_path}/{self.image_list[i]}')\n",
    "                image = preprocess(image, return_tensors=\"pt\")\n",
    "                image = image[\"pixel_values\"]\n",
    "                with torch.no_grad():\n",
    "                    embedding = model.get_image_features(image)\n",
    "                    embedding = embedding.reshape(embedding.shape[1]).tolist()\n",
    "            except:\n",
    "                print(f'{i} is failed')\n",
    "            else:\n",
    "                img_clip = {'img': self.image_list[i], 'clip': embedding}\n",
    "                with open('clip.json', 'a') as clip:\n",
    "                    json.dump(img_clip, clip)\n",
    "                    clip.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building CLIP-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wfUFWX9nsCC0"
   },
   "outputs": [],
   "source": [
    "#We form image array with all image files\n",
    "dataset = Dataset(dir_path)\n",
    "img_list = dataset.img_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build CLIP vectors using obtained image array\n",
    "clip = CLIP(img_list)\n",
    "features = clip.get_features()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
