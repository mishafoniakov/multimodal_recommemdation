# multimodal_recommemdation
Application of multimodal models for the image recommendation systems

Welcome to the GitHub page of my project 'Application of multimodal models for the image recommendation systems'

This repository consists of two parts. Part 1 contains only results of my project. Part 2 contains all steps of my project.

## 1. Results

### Step 1. Download image dataset and text dataset
[Yandex Image Dataset](https://disk.yandex.ru/d/3owCpPC5nd3BAQ)
[Yandex Text Dataset](https://github.com/mishafoniakov/multimodal_recommendation/blob/main/step_1/01_image_text_dataset.json)

### Step 2. Run the notebok
[Results](https://github.com/mishafoniakov/multimodal_recommendation/blob/main/step_6/06_results.ipynb)

## 1. All steps

### Step 1. Download image dataset and text dataset
[Yandex Image Dataset](https://disk.yandex.ru/d/3owCpPC5nd3BAQ)

### Step 2. Building CLIP&BERT vectors. Run these notebooks contineously
[CLIP Preparing](https://github.com/mishafoniakov/multimodal_recommendation/blob/main/step_1/01_img_dataset.ipynb)

[BERT Preparing](https://github.com/mishafoniakov/multimodal_recommendation/blob/main/step_2/02_txt_dataset.ipynb)

As a result, you should have this file
[CLIP&BERT Dataset](https://disk.yandex.ru/d/zBu38Dzt0c1_HA)

### Step 3. Creating clusters for CLIP&BERT embeddings
[Creating clusters](https://github.com/mishafoniakov/multimodal_recommendation/blob/main/step_3/03_clusters_pipeline.ipynb)

As a result, you should have this dataframe with paired images, accordin to clusters
[Paired images dataframe]